{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 3 timesteps\n",
      "Episode finished after 4 timesteps\n",
      "Episode finished after 5 timesteps\n",
      "Episode finished after 6 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 3 timesteps\n",
      "Episode finished after 4 timesteps\n",
      "Episode finished after 5 timesteps\n",
      "Episode finished after 6 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 3 timesteps\n",
      "Episode finished after 4 timesteps\n",
      "Episode finished after 5 timesteps\n",
      "Episode finished after 6 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 3 timesteps\n",
      "Episode finished after 4 timesteps\n",
      "Episode finished after 5 timesteps\n",
      "Episode finished after 6 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 3 timesteps\n",
      "Episode finished after 4 timesteps\n",
      "Episode finished after 5 timesteps\n",
      "Episode finished after 6 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 3 timesteps\n",
      "Episode finished after 4 timesteps\n",
      "Episode finished after 5 timesteps\n",
      "Episode finished after 6 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 3 timesteps\n",
      "Episode finished after 4 timesteps\n",
      "Episode finished after 5 timesteps\n",
      "Episode finished after 6 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 3 timesteps\n",
      "Episode finished after 4 timesteps\n",
      "Episode finished after 5 timesteps\n",
      "Episode finished after 6 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 24 timesteps\n",
      "Episode finished after 25 timesteps\n",
      "Episode finished after 26 timesteps\n",
      "Episode finished after 27 timesteps\n",
      "Episode finished after 28 timesteps\n",
      "Episode finished after 29 timesteps\n",
      "Episode finished after 30 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 32 timesteps\n",
      "Episode finished after 33 timesteps\n",
      "Episode finished after 34 timesteps\n",
      "Episode finished after 35 timesteps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# 超参数\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # 最优选择动作百分比\n",
    "GAMMA = 0.9                 # 奖励递减参数\n",
    "TARGET_REPLACE_ITER = 100   # Q 现实网络的更新频率\n",
    "MEMORY_CAPACITY = 2000      # 记忆库大小\n",
    "env = gym.make('CartPole-v0')   # 立杆子游戏\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = env.action_space.n  # 杆子能做的动作\n",
    "N_STATES = env.observation_space.shape[0]   # 杆子能获取的环境信息数\n",
    "\n",
    "# 神经网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 10)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(10, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "        \n",
    "# DQN 体系\n",
    "class DQN(object):\n",
    "    # 建立 target net 和 eval net 还有 memory\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "\n",
    "        self.learn_step_counter = 0     # 用于 target 更新计时\n",
    "        self.memory_counter = 0         # 记忆库记数\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # 初始化记忆库\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    # torch 的优化器\n",
    "        self.loss_func = nn.MSELoss()   # 误差公式\n",
    "    \n",
    "    # 根据环境观测值选择动作的机制\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # 这里只输入一个 sample\n",
    "        if np.random.uniform() < EPSILON:   # 选最优动作\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()[0]     # return the argmax\n",
    "        else:   # 选随机动作\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "    \n",
    "    # 存储记忆\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # 如果记忆库满了, 就覆盖老数据\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    # target 网络更新\n",
    "    # 学习记忆库中的记忆\n",
    "    def learn(self):\n",
    "        # target net 参数更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # 抽取记忆库中的批数据\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # 针对做过的动作b_a, 来选 q_eval 的值, (q_eval 原本有所有动作的值)\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # q_next 不进行反向传递误差, 所以 detach\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0]   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        # 计算, 更新 eval net\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# 训练\n",
    "dqn = DQN() # 定义 DQN 系统\n",
    "\n",
    "for i_episode in range(400):\n",
    "    s = env.reset()\n",
    "    time_count = 1\n",
    "    while True:\n",
    "        # env.render()    # 显示实验动画\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        # 选动作, 得到环境反馈\n",
    "        s_, r, done, info = env.step(a)\n",
    "        \n",
    "        x, x_dot, theta, theta_dot = s_   # 细分开, 为了修改原配的 reward\n",
    "\n",
    "        # x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少\n",
    "        # theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高\n",
    "\n",
    "        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2   # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率\n",
    "\n",
    "        # 存记忆\n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn() # 记忆库满了就进行学习\n",
    "\n",
    "        if done:    # 如果回合结束, 进入下回合\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "        time_count += 1\n",
    "        \n",
    "        if i_episode % 50 == 0:\n",
    "            print(f\"Episode finished after {time_count + 1} timesteps\")\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
