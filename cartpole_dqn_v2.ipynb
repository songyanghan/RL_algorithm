{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num\tObservation               Min             Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                -24 deg         24 deg\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "    Reward:\n",
    "        Original reward is 1 for every step taken, including the termination step\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average reward is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hypterparameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # 最优选择动作百分比\n",
    "# GAMMA = 0.9                 \n",
    "EPS_START = 0.9            # Determine epsilon adaptively\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_REPLACE_ITER = 100   # Q 现实网络的更新频率\n",
    "MEMORY_CAPACITY = 2000      # 记忆库大小\n",
    "env = gym.make('CartPole-v0')   # 立杆子游戏\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = env.action_space.n  # 杆子能做的动作\n",
    "N_STATES = env.observation_space.shape[0]   # 杆子能获取的环境信息数\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 10)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(10, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "        \n",
    "# DQN framework\n",
    "class DQN(object):\n",
    "    # 建立 target net 和 eval net 还有 memory\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = QNetwork().to(device), QNetwork().to(device)\n",
    "\n",
    "        self.learn_step_counter = 0     # 用于 target 更新计时\n",
    "        self.memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    # torch 的优化器\n",
    "        self.loss_func = nn.MSELoss()   # 误差公式\n",
    "    \n",
    "    # 根据环境观测值选择动作的机制\n",
    "    def choose_action(self, x):\n",
    "        global steps_done\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        # 这里只输入一个 sample\n",
    "        if np.random.uniform() > eps_threshold:   # 选最优动作\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].numpy()[0]     # return the argmax\n",
    "        else:   # 选随机动作\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "    \n",
    "    # target 网络更新\n",
    "    # 学习记忆库中的记忆\n",
    "    def learn(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # target net 参数更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # 抽取记忆库中的批数据\n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        b_memory = Transition(*zip(*batch))\n",
    "        b_s = torch.FloatTensor(b_memory.state)\n",
    "        b_a = torch.LongTensor(b_memory.action).unsqueeze(1)\n",
    "        b_r = torch.FloatTensor(b_memory.reward).unsqueeze(1)\n",
    "        b_s_ = torch.FloatTensor(b_memory.next_state)\n",
    "\n",
    "        # 针对做过的动作b_a, 来选 q_eval 的值, (q_eval 原本有所有动作的值)\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # q_next 不进行反向传递误差, 所以 detach\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0]   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        # 计算, 更新 eval net\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 16 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/v_hansongyang/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 11 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 24 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 26 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 30 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 30 timesteps\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "dqn = DQN() # 定义 DQN 系统\n",
    "\n",
    "for i_episode in range(1000):\n",
    "    s = env.reset()\n",
    "    time_count = 1\n",
    "    while True:\n",
    "        # env.render()    # 显示实验动画\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        # 选动作, 得到环境反馈\n",
    "        s_, r, done, info = env.step(a)\n",
    "        \n",
    "        x, x_dot, theta, theta_dot = s_   # 细分开, 为了修改原配的 reward\n",
    "\n",
    "        # x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少\n",
    "        # theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高\n",
    "        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2   # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率\n",
    "\n",
    "        # 存记忆\n",
    "        dqn.memory.push(s, a, r, s_)\n",
    "        # learning\n",
    "        dqn.learn()\n",
    "\n",
    "        if done:    # 如果回合结束, 进入下回合\n",
    "            break\n",
    "\n",
    "        s = s_\n",
    "        time_count += 1\n",
    "    if i_episode % 50 == 0:\n",
    "        print(\"Episode finished after {} timesteps\".format(time_count+1))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
